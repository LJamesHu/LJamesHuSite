<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description></description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 06 May 2021 13:48:00 -0700</pubDate>
    <lastBuildDate>Thu, 06 May 2021 13:48:00 -0700</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Musings on Job Hunting</title>
        <description>&lt;p&gt;Job hunting is terrible. Whether it’s your first or your company sucks or some other circumstances, looking for a job usually proves to be a miserable experience. It’s so bad that there are &lt;a href=&quot;https://www.cnbc.com/2018/08/01/google-job-search-is-using-ai-to-make-job-searching-a-whole-lot-easier.html&quot;&gt;multiple&lt;/a&gt; &lt;a href=&quot;https://www.linkedin.com/jobs/&quot;&gt;companies&lt;/a&gt; &lt;a href=&quot;https://www.indeed.com/&quot;&gt;trying&lt;/a&gt; &lt;a href=&quot;https://www.vettery.com/&quot;&gt;to solve&lt;/a&gt; &lt;a href=&quot;https://woo.io/candidates/candidate&quot;&gt;job&lt;/a&gt; &lt;a href=&quot;https://triplebyte.com/&quot;&gt;hunting&lt;/a&gt; &lt;a href=&quot;https://adevait.com/&quot;&gt;and&lt;/a&gt; &lt;a href=&quot;https://www.theinformation.com/articles/job-site-hired-once-valued-at-500-million-discusses-winding-down&quot;&gt;failing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Not only that, but for the vast majority of people, unless you went to a top 10 school, worked at one of the FAANGs or somewhere equally as reputable, you’re more than likely getting at best a 10% call back rate, getting rejected 90% of the time, without even knowing why. It’s even worse if you don’t have much experience or trying to pivot.&lt;/p&gt;

&lt;p&gt;In response to that, I think the number one strategy is applying to a ton of jobs. High volume is by far the best strategy. The first time after my data science bootcamp, I applied to over 300 places and only interviewed on-site at 5 or 6, and in the end, only got an offer at 2. Your rejection rate will be 98%. Even after 2 years of work experience, I am still at a ~95% rejection rate. In the end, the only thing I regret is not applying in greater volume. Remember, them not accepting you is the norm, and there is no way around that.&lt;/p&gt;

&lt;p&gt;Tied to that, you just have to apply. If you’re still writing custom cover letters for every job that you apply to, you need to cut back significantly. Out of all the positions I’ve applied to, only 1 company out of 500+ has ever mentioned my cover letter when talking to a real decision maker, and from all the interviewing processes I’ve seen, no real decision maker is given your cover letter because they’re mostly fluff. I would say at most 20% of applications should have a customized cover letter, the rest get a stock cover letter with some minor adjustments. Writing customized cover letters massively slows down your application process, grinding your applications to a halt. Not to mention the massive psychological impact of having spent 15-30 minutes writing a cover letter, for them to completely ghost you.&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Mar 2019 00:00:00 -0800</pubDate>
        <link>/2019/03/01/musings-on-job-hunting.html</link>
        <guid isPermaLink="true">/2019/03/01/musings-on-job-hunting.html</guid>
        
        
        <category>technical</category>
        
      </item>
    
      <item>
        <title>DOTA 2 Guides Monte Carlo Simulation</title>
        <description>&lt;p&gt;Torte de Lini, an old friend that I’ve also been working with to set up technical infrastructure for his &lt;a href=&quot;/2015/05/01/dota-2-hero-builds-project.html&quot;&gt;Dota 2 Heroes Build Project&lt;/a&gt;, approached me with a new, interesting question. Given that his guides have so many subscribers, how often, and how many people use his guides in a game? While we can’t get the exact numbers, we can come up with a rough estimation given a bunch of numbers that he’s been collecting.&lt;/p&gt;

&lt;p&gt;Given that we know that how many games are played daily (dota.rgp.io), how many games are played for each hero (dotabuff.com), and how many games for a specific hero use a Torte de Lini guide by day (available in-game), we can then get an estimate of how many games have at least one person using a Torte de Lini guide.&lt;/p&gt;

&lt;p&gt;This was calculated and estimated by using a Monte Carlo simulation method. While it could theoretically be calculated to exact precision given that we have the percents, there are a possible 81,572,506,886,508 possible team combinations, which is an unrealistic amount of calculations required for very little upside. The Monte Carlo simulation created teams of 10 different heroes, based on play rate and calculated the probability of that person using a guide given the numbers provided. This was then run 100,000 times and stored.&lt;/p&gt;

&lt;p&gt;Across multiple tests, the percents were fairly stable. So the Monte Carlo simulation shows that roughly 17% of games have no one in the game using a guide, but in the other 83%, at least 1 person uses a guide, which is a pretty remarkable number.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/guide_usage_distribution.png&quot; alt=&quot;Guide Usage Distribution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To semi-validate this data, given the numbers we have, it seems like roughly 15% of all games across all heroes use a Torte de Lini guide, which means that for any individual hero, 85% of games do not. If we take that basic assumption given all heroes are picked equally, the probability of a game having no one using a guide is .85 ^ 10 or 19.68%. Not too far off from what our Monte Carlo simulation shows, so the results look fairly reasonable. So even though only 15% of games played on any individual hero uses a Torte de Lini guide, the combined probability of among 10 people, at least 1 person using a guide is around 83%.&lt;/p&gt;

&lt;p&gt;Some assumptions are made that are not representative of the real world. Heroes were selected at random, whereas more realistically, you would assume some level of team composition. There is also the assumption that players will use guides at random, whereas the truth is probably less skilled players use guides more often than skilled players. So realistically, more than 17% of games will have no one using a guide, but there is no better way to calculate this skill based guide usage.&lt;/p&gt;

&lt;p&gt;Given the prevalence of the guides’ usage, the results weren’t too surprising. His guides are some of the most popular tools in the Dota 2 community, having reached pretty far and wide for a free project inside Dota 2.&lt;/p&gt;

&lt;p&gt;This was posted on &lt;a href=&quot;http://tortedelini.com/2018/12/10/5-years-350-million-2018-dota-builds-project-year-review/&quot;&gt;his blog site&lt;/a&gt; for his 5 year anniversary of the Dota 2 Hero Builds Project.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/LJamesHu/LJamesHuSite/blob/master/assets/files/Dota%202%20Monte%20Carlo.ipynb&quot;&gt;Dota 2 Monte Carlo ipynb&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Feb 2019 00:00:00 -0800</pubDate>
        <link>/2019/02/01/dota-2-guides-monte-carlo-simulation.html</link>
        <guid isPermaLink="true">/2019/02/01/dota-2-guides-monte-carlo-simulation.html</guid>
        
        
        <category>technical</category>
        
        <category>python</category>
        
      </item>
    
      <item>
        <title>Making A New Personal Site - Jekyll Edition</title>
        <description>&lt;p&gt;Profile sites are basically a person’s way of interacting and showing the outside world the kind of work you’ve done. I’ve had a profile site for a while but never really “loved” it. I’ve had a website since 2016, to serve as a place to link to when showing people work and also have an email that isn’t &lt;strong&gt;@gmail.com&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;I had a site made by my good friend &lt;a href=&quot;https://websitedaytona.com/&quot;&gt;Zak Tan&lt;/a&gt; and it worked for me at the time, but didn’t quite meet my vision, and I wasn’t sure how to move on from there. Most sites are very image heavy, but as a data scientist, my work really wasn’t image heavy overall. I wanted images to play back seat to content.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/old_wp_site_2016.png&quot; alt=&quot;Old LJamesHu Wordpress Site&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Technologically, it also felt a little problematic for me. It was based off Wordpress, which is a fine technology stack, but didn’t work for me. I didn’t really want to keep hosting a Wordpress server that was both cumbersome to manage, but also had so many extra features I wasn’t taking advantage of, and was too difficult for me to modify. If I wanted to change things, the code was a little too bespoke for me to change without learning the full stack. I also had little to no desire to learn PHP, arguably a dying language, and definitely not something really used by data scientists.&lt;/p&gt;

&lt;p&gt;Looking around at the options, a lot of things pointed to using Jekyll. While it’s a bit played out, Jekyll is the language of tech people, who don’t need a WYSIWG editor, who want to get things done quickly, who want to host on GitHub Pages for free.&lt;/p&gt;

&lt;p&gt;After having done some work with getting the &lt;a href=&quot;/2017/02/01/extra-life-2016.html&quot;&gt;Luminar Extra Life Site&lt;/a&gt; online, I felt like GitHub would be the right place to have host my new site.&lt;/p&gt;

&lt;p&gt;Additionally, I drew a lot of inspiration from &lt;a href=&quot;http://www.lilchen.com/blog/site-redesign-process/&quot;&gt;Lilian Chen&lt;/a&gt;. The website style was very close to what I wanted although far more artistic than I had the ability to continuously do. I wanted to try and tweak her design into something that I would love to use. But it did give me a sense of what I wanted and as a not designer, her breakdown of her approach helped me also refine what I was looking for, and what I didn’t really care about.&lt;/p&gt;

&lt;p&gt;After all of that, Jekyll seemed like the obvious choice. I actually initially tried copying her code directly, but it clearly wasn’t designed to be completely understood by a beginner, or to even be copied. Which is fair, unless you’re planning on sharing something, there really isn’t a huge point in generalizing things for other people.&lt;/p&gt;

&lt;p&gt;That, combined with Jekyll being relatively poorly documented in my opinion, and it’s just difficult to get things set up. I had to start with something… simpler so I took a look at free Jekyll templates, and immediately &lt;a href=&quot;https://github.com/wowthemesnet/mediumish-theme-jekyll/&quot;&gt;Mediumish&lt;/a&gt;, one of the most used Jekyll templates, spoke to me. Visually it wasn’t quite what I wanted, much more image forward, but structurally it was designed to be copied and modified, and I felt like I could take it the last mile.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mediumish-jekyll-template.png&quot; alt=&quot;Mediumish Jekyll Template&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But in order to reach the certain vision I wanted to implement, it proved to be fairly difficult. Jekyll tech is based off of a combination of Ruby, Liquid templating, and Markdown for the last mile. For someone who has never done this before… it proved… confusing. Most of my experience programming would be in the form of analysis, not design so all of this was new to me.&lt;/p&gt;

&lt;p&gt;Ultimately, took me a lot of work, lots of googling, but it got to the site that you see now. Now this may change in the future, but this seems pretty good to me. It’s not a image forward, but rather more text forward and really serves some of the things that I immediately wanted.&lt;/p&gt;
</description>
        <pubDate>Sat, 01 Dec 2018 00:00:00 -0800</pubDate>
        <link>/2018/12/01/making-a-new-personal-site.html</link>
        <guid isPermaLink="true">/2018/12/01/making-a-new-personal-site.html</guid>
        
        
        <category>technical</category>
        
      </item>
    
      <item>
        <title>Hello World Version 2: Electric Boogaloo</title>
        <description>&lt;p&gt;Hello World Version 2: Electric Boogaloo&lt;/p&gt;

&lt;p&gt;This is basically marking the relaunch of the site. Given that Wordpress site felt a little outdated in terms of design, structure, and my own ability to customize it, I decided to move towards a Jekyll solution hosted on Github Pages. Part test, part just fun project to revamp and possibly make a real blog that I can be proud of.&lt;/p&gt;
</description>
        <pubDate>Mon, 01 Oct 2018 00:00:00 -0700</pubDate>
        <link>/2018/10/01/hello-world-version-2-electric-boogaloo.html</link>
        <guid isPermaLink="true">/2018/10/01/hello-world-version-2-electric-boogaloo.html</guid>
        
        
        <category>technical</category>
        
      </item>
    
      <item>
        <title>Extra Life 2016</title>
        <description>&lt;p&gt;Extra Life is a major yearly fundraising event where the proceeds go to branches of the Children’s Miracle Network Hospitals. I did this with a group of friends that I was playing with in the game The Secret World participated in a relatively large 24 hour event for the Extra Life charity that involved hundreds of players in game. Our Extra Life team managed to donate a combined &lt;strong&gt;$1,645 dollars across 51 donations&lt;/strong&gt; to charity and was among the top 15% of teams that year!&lt;/p&gt;

&lt;p&gt;This was more of a fun one off kind of thing I did. Prior to this I’ve never really used GitHub Pages to create a website, and wanted to make something useful to help our team, and do some experimentation with how far I could take a site on GitHub Pages and what kind of simple things I can do with some simple JavaScript and Bootstrap. It’s not too hard to set it up but it takes a while to get used to modifying JavaScript and Bootstrap code to fit exactly what you need when you’ve never touched Javascript before or used much Bootstrap.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ljameshu.github.io/LuminarExtraLife/&quot;&gt;Luminar Extra Life 2016 Site&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/LJamesHu/LuminarExtraLife&quot;&gt;Luminar Extra Life Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I also created some python code to randomize some in game items into equally sized groups of three items. Pretty simple but useful code as it allowed us to get new items and still randomize things well.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/LJamesHu/LuminarExtraLife/blob/master/itembundler.py&quot;&gt;Luminar Extra Life Code&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 01 Feb 2017 00:00:00 -0800</pubDate>
        <link>/2017/02/01/extra-life-2016.html</link>
        <guid isPermaLink="true">/2017/02/01/extra-life-2016.html</guid>
        
        
        <category>technical</category>
        
      </item>
    
      <item>
        <title>Tax Rates Compared to Corruption</title>
        <description>&lt;p&gt;Reddit tends to be a highly liberal place, supporting the idea of higher taxes and more benefits as something good for society and the economy. While this is sometimes true it isn’t always. In particular, one person &lt;a href=&quot;https://np.reddit.com/r/bestof/comments/3p0or5/danish_doctor_drops_devastating_dissertation_on/cw2nuz7&quot;&gt;stated&lt;/a&gt; that Scandinavia’s high taxes and low corruption are indicative of a larger trend of high taxes correlating with less corruption. I wanted to explore this trend, especially since from my own knowledge, countries like Italy are renown for their incredibly high corruption, and certainly have higher taxes than the United States. While the sentiment seems nice, I couldn’t see this necessarily being true. Didn’t make enough sense so I did a simple investigation. The line drawn through is a linear regression but with very poor fit so it must be taken with a grain of salt.&lt;/p&gt;

&lt;p&gt;As you can see, the conclusion they drew is not necessarily true. It looks almost wholly random and inconsistent. The narrative posed sounds nice but really needs investigation before it can be stated as fact. I used CPI from &lt;a href=&quot;http://www.transparency.org/cpi2014/results/&quot;&gt;Transparency International&lt;/a&gt; and OECD Tax Wedge information from &lt;a href=&quot;http://www.keepeek.com/Digital-Asset-Management/oecd/taxation/taxing-wages-2015_tax_wages-2015-en#page20&quot;&gt;the OECD Taxing Wages paper&lt;/a&gt;. This was then compiled in Excel for a quick and dirty visualization.&lt;/p&gt;

&lt;p&gt;Afterwards I &lt;a href=&quot;https://np.reddit.com/r/dataisbeautiful/comments/3p5awo/tax_rates_vs_corruptions_perceptions_index_oc/&quot;&gt;posted it on reddit&lt;/a&gt;, which was somewhat received well by the DataIsBeautiful subreddit and gave me some valuable input on visualization design.&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Dec 2016 00:00:00 -0800</pubDate>
        <link>/2016/12/01/tax-rates-compared-to-corruption.html</link>
        <guid isPermaLink="true">/2016/12/01/tax-rates-compared-to-corruption.html</guid>
        
        
        <category>technical</category>
        
      </item>
    
      <item>
        <title>Heart-icle</title>
        <description>&lt;p&gt;Heart-icle is a project to solve the problem of most summaries out there. There is such a large quantity of news content out there to read and not enough time to actually read them. Summaries on sites right now are designed to specifically NOT give you the information you want, to entice you to click in order to actually get into the meat and bones.&lt;/p&gt;

&lt;p&gt;My proposal was to make a better, more intelligent summarizer. While there are some acceptable summarizers such as TextRank, which use a graph-based ranking model for text processing, somewhat analogous to Google’s PageRank, but for text. However it isn’t trained for it, just happens to leverage graphs intelligently to come up with semi-useful summary extractions, that may not be meaningful. I wanted to make a better summarizer.&lt;/p&gt;

&lt;p&gt;The core of this idea came from observing articles on the blogging site Medium. Users on the site can highlight parts of the article, and the most highlighted segment is the “Top Highlight”. The more I read articles on Medium, the more I felt that Medium highlights were actually quite representative of the article in one line. Given that I can scrape Medium articles, I can functionally have a labeled data set to train a model and use that to come up with a model specifically trained for summarization.&lt;/p&gt;

&lt;p&gt;To do this I had to scrape over thirty thousand articles, on both Medium to get the articles and the relevant highlights, and also the Atlantic, to build up a meaningful corpus of news and article language for analysis. This was done through a combination of Selenium Webdriver and BeautifulSoup, which worked well in terms of downloading and processing a large amount of web data and stripping it down to the core article information.&lt;/p&gt;

&lt;p&gt;To generate the summaries themselves, there were a couple of models that I was looking at, but I ended up zeroing in on Doc2Vec. It’s both cutting edge, but also a new application of the tool. The canonical example for Word2Vec is the relationship between King and Queen is the same as Man and Woman.&lt;/p&gt;

&lt;p&gt;I wanted to take this conceptual framework and apply it to articles and summaries. If I take an article and compare it to its summary, I should be able to figure out what the summarization relationship is and then utilize that to find the summary for un-summarized articles.&lt;/p&gt;

&lt;p&gt;The results were fairly interesting, with a decent accuracy rate at roughly 10%, good for a short project trying to answer a complex question, but ultimately not good enough to feel like you could comfortably use it to generate summaries, and thus relegated to Github.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/LJamesHu/Heart-icle&quot;&gt;Heart-icle Code on Github&lt;/a&gt;&lt;/p&gt;

&lt;object data=&quot;/assets/files/Heart-icle.pdf&quot; width=&quot;100%&quot; height=&quot;600&quot; type=&quot;application/pdf&quot;&gt;&lt;/object&gt;

&lt;p&gt;&lt;a href=&quot;/assets/files/Heart-icle.pdf&quot;&gt;Link to Download PDF Presentation&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 01 Aug 2016 00:00:00 -0700</pubDate>
        <link>/2016/08/01/hearticle.html</link>
        <guid isPermaLink="true">/2016/08/01/hearticle.html</guid>
        
        
        <category>technical</category>
        
        <category>python</category>
        
      </item>
    
      <item>
        <title>Yelp Data Project</title>
        <description>&lt;p&gt;Yelp is the premier site of restaurant review and rating data in the United States. Although Yelp has an API, it’s not quite sufficient for any meaningful analysis.&lt;/p&gt;

&lt;p&gt;This project was for a Practical Data Science class that was intended for MBA students but was fascinating across the board and one of my favorite classes during my time at New York University. For this group project, I mainly focused on data collection/scraping strategies and mass collection and storage of data, although I did help with the analysis portions and the overall writeup.&lt;/p&gt;

&lt;p&gt;This report shows the difficulties in the collection of the data, the analysis, and the visualization strategies we used for data. We found some interesting conclusions, one of which is that restaurants offering a sponsored deal typically perform poorly comparatively.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/LJamesHu/Yelp-Data-Scraper&quot;&gt;Data Scraping Code on Github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The code was written in Python and relied heavily on the BeautifulSoup library. Analysis and visualizations were done using matplotlib and Tableau.&lt;/p&gt;

&lt;object data=&quot;/assets/files/YelpDataProject.pdf&quot; width=&quot;100%&quot; height=&quot;600&quot; type=&quot;application/pdf&quot;&gt;&lt;/object&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;/assets/files/YelpDataProject.pdf&quot;&gt;Link to Download PDF Report&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/files/yelpr_East_Village.csv&quot;&gt;Sample Data&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 01 Jun 2015 00:00:00 -0700</pubDate>
        <link>/2015/06/01/yelp-data-project.html</link>
        <guid isPermaLink="true">/2015/06/01/yelp-data-project.html</guid>
        
        
        <category>technical</category>
        
        <category>python</category>
        
      </item>
    
      <item>
        <title>Dota 2 Hero Builds Project</title>
        <description>&lt;p&gt;The Dota 2 Hero Builds Project is a project my friend &lt;a href=&quot;https://twitter.com/tortedelini&quot;&gt;Michael Cohen&lt;/a&gt; started. I provide a lot of the technical support to his project so he can get it running smoothly. I host his site &lt;a href=&quot;http://tortedelini.com/&quot;&gt;Armchair Athleticism / Torte De Lini.com&lt;/a&gt; on my own VPS server on a WordPress installation. I have also provided significant data analysis assistance, being a big part of his &lt;a href=&quot;http://tortedelini.com/2018/12/10/5-years-350-million-2018-dota-builds-project-year-review/&quot;&gt;5 years, 350 million&lt;/a&gt;, &lt;a href=&quot;http://tortedelini.com/2018/02/10/4-years-275-million-2017-dota-builds-project-year-review/&quot;&gt;4 years, 275 million&lt;/a&gt;, &lt;a href=&quot;http://tortedelini.com/2016/11/17/3-years-170-million-dota-builds-project-year-review/&quot;&gt;3 years, 170 million&lt;/a&gt;, &lt;a href=&quot;http://tortedelini.com/2015/10/28/2-years-100-million-dota-builds-project-overview/&quot;&gt;2 years, 100 million&lt;/a&gt;, and &lt;a href=&quot;http://tortedelini.com/2014/11/17/1-year-40-million-dota-builds-project-overview/&quot;&gt;1 Year, 40 million&lt;/a&gt; subscriptions benchmark articles and analysis, assisting in the data visualization and writing. My work with him has helped him secure commercial sponsorships.&lt;/p&gt;

&lt;p&gt;To assist in getting statistics for all of the over 100 guides, I created a data collection scraper and analyzer for near instant calculation and parsing of the data. Normally it would take up to an hour to manually calculate and pull data every 2 weeks, which was automated into a single script that collects all the data and outputs a csv for intake.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/LJamesHu/Dota2GuideInfoScraper&quot;&gt;Dota 2 Guide Info Scraper on GitHub&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The code was written in Python and relied heavily on the BeautifulSoup library to pull information from each of this guides. This was then all compiled into an exe file so Torte de Lini could quickly use it without any Python setup or additional files.&lt;/p&gt;

&lt;object data=&quot;/assets/files/guideData-2015-11-14-02-18-12.pdf&quot; width=&quot;100%&quot; height=&quot;600&quot; type=&quot;application/pdf&quot;&gt;&lt;/object&gt;

&lt;p&gt;&lt;a href=&quot;/assets/files/guideData-2015-11-14-02-18-12.csv&quot;&gt;Link to Download Sample CSV Data&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In addition to that, since Torte De Lini has many guides, so it is rather annoying to get people to subscribe to, favorite, or rate his guides. I learned how to make bookmarklets and designed a bookmarklet that allowed people to like, favorite and subscribe to all of his guides in one button press. It was a rather interesting project since I was relatively unfamiliar with bookmarklets and had to figure out how to set it up to work correctly.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/LJamesHu/Dota2SubLikeFav&quot;&gt;Dota2SubLikeFav Bookmarklets on GitHub&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In order to answer the question of how many games have at least one person who used a guide by Torte De Lini, I ran a Monte Carlo simulation to estimate this.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/2019/02/01/dota-2-guides-monte-carlo-simulation.html&quot;&gt;DOTA 2 Guides Monte Carlo Simulation&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 01 May 2015 00:00:00 -0700</pubDate>
        <link>/2015/05/01/dota-2-hero-builds-project.html</link>
        <guid isPermaLink="true">/2015/05/01/dota-2-hero-builds-project.html</guid>
        
        
        <category>technical</category>
        
        <category>python</category>
        
      </item>
    
      <item>
        <title>NYU Projects</title>
        <description>&lt;p&gt;In my four years as an undergrad at New York University, I have taken a wide variety of classes that have given me broad business knowledge and skills. These are an overall set of worthwhile or interesting projects from my time at New York University that show analysis, research skills, writing skills, and proficiency across multiple business areas.&lt;/p&gt;

&lt;p&gt;Two projects stand out in particular as being analytical and very business focused.&lt;/p&gt;

&lt;h2 id=&quot;smuckers-case-study&quot;&gt;Smuckers Case Study&lt;/h2&gt;

&lt;p&gt;This was a two week case study for PriceWaterhouseCoopers Industry Analysis Consulting Case Competition. The goal was to outline the strategic challenges and opportunities of a selected Fortune 500 company and come up with a plan to move the company forward.&lt;/p&gt;

&lt;p&gt;Initially we were undecided and needed to identify key Fortune 500 companies that we thought had strong potential. To do this, we had to parse the list of Fortune 500 companies for potential opportunities. Unfortunately the Fortune 500 data was locked up in an inaccessible form, paginated, and difficult read, analyze, or sort any aspects of the company, even though the information was there. So one of the first things I did was organize the Fortune 500 into more manipulable CSV format to allow for better analysis than the current Fortune 500 site.&lt;/p&gt;

&lt;p&gt;To simplify this, I created a script in Python with the BeautifulSoup library to scrape all the data in the site, then put it into a csv format. This made the data accessible and manipulable for analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/LJamesHu/Fortune500Scraper&quot;&gt;Fortune 500 Scraping Code on Github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/files/nyu/Fortune500Data.csv&quot;&gt;Download Scraped Fortune 500 Data&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For our company, we chose J.M. Smuckers and came up with a comprehensive and innovative plan to drive sales through Smuckers’ core values.&lt;/p&gt;

&lt;object data=&quot;/assets/files/nyu/SmuckersCaseStudy.pdf&quot; width=&quot;100%&quot; height=&quot;600&quot; type=&quot;application/pdf&quot;&gt;&lt;/object&gt;

&lt;p&gt;&lt;a href=&quot;/assets/files/nyu/SmuckersCaseStudy.pptx&quot;&gt;Link to Download Powerpoint&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;sovereign-bank-risk-study&quot;&gt;Sovereign Bank Risk Study&lt;/h2&gt;

&lt;p&gt;This study was the final project for the class Risk Management in Financial Institutions.&lt;/p&gt;

&lt;p&gt;The interesting part about this was how it took many of the concepts that we learned in class and applied them in a real life scenario. After taking data from the FDIC website, we had to process and apply strategies and our knowledge to the data to obtain a reasonable analysis of the bank.&lt;/p&gt;

&lt;p&gt;The project required a thorough analysis covering various risk exposures such as liquidity and credit risk, capital adequacy measures, profitability, and risk management tools that were being used. These gave interesting insights into the bank and also were doubly interesting in how they revealed the rather jarring changes due to the financial crisis in 2008.&lt;/p&gt;

&lt;object data=&quot;/assets/files/nyu/SovereignBankAnalysis.pdf&quot; width=&quot;100%&quot; height=&quot;600&quot; type=&quot;application/pdf&quot;&gt;&lt;/object&gt;

&lt;p&gt;&lt;a href=&quot;/assets/files/nyu/SovereignBankAnalysis.pdf&quot;&gt;Link to Download PDF Report&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;other-projects&quot;&gt;Other Projects&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/06/01/yelp-data-project.html&quot;&gt;Practical Data Science - Yelp Data Project&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/assets/files/nyu/GlobalPerspectivesMexico.pdf&quot;&gt;Global Perspectives on Enterprise Systems – Mexico: Problems, Strengths and the Path&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/assets/files/nyu/GlobalPerspectivesIndiaPresentation.pdf&quot;&gt;Global Perspectives on Enterprise Systems – India Presentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/assets/files/nyu/GrupoDammAnalysis.pdf&quot;&gt;International Studies Program – Grupo Damm Analysis Presentation&lt;/a&gt;: A class presentation structured around learning about international business, featuring a one week trip to Barcelona to learn and study the culture.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/assets/files/nyu/EconomicsOfPhilanthropy.pdf&quot;&gt;The Economics of Philanthropy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/assets/files/nyu/EmergingMarketsPeru.pdf&quot;&gt;Topics in Emerging Financial Markets – Peru Presentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/assets/files/nyu/Canon.pdf&quot;&gt;Canon Presentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/assets/files/nyu/CanonCSR.pdf&quot;&gt;Canon Corporate Social Responsibility Presentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/assets/files/nyu/CanonVSmartphones.pdf&quot;&gt;Canon and the Smartphone Threat&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/assets/files/nyu/KindleFirePublishers.pdf&quot;&gt;Kindle Fire Presentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 01 Apr 2015 00:00:00 -0700</pubDate>
        <link>/2015/04/01/nyu-projects.html</link>
        <guid isPermaLink="true">/2015/04/01/nyu-projects.html</guid>
        
        
        <category>business</category>
        
        <category>technical</category>
        
      </item>
    
  </channel>
</rss>
